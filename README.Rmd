---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# outlieR

<!-- badges: start -->
[![R-CMD-check](https://github.com/fabiandistler/outlieR/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/fabiandistler/outlieR/actions/workflows/R-CMD-check.yaml)
[![Codecov test coverage](https://codecov.io/gh/fabiandistler/outlieR/graph/badge.svg)](https://app.codecov.io/gh/fabiandistler/outlieR)
<!-- badges: end -->

> Automatic Outlier Detection Using Isolation Forests

[Package Homepage]https://fabiandistler.github.io/outlieR/

## Overview

`outlieR` provides a simple, powerful API for automatic outlier detection in tabular data. The package uses Isolation Forests (via `isotree`) with automatic hyperparameter tuning and delivers detailed feature-level diagnostics.

### Main Features

âœ¨ **Simple API** - One function for most use cases
ðŸŽ¯ **Automatic Tuning** - Grid Search, Random Search, or Bayesian Optimization
ðŸ“Š **Detailed Diagnostics** - Feature-level outlier analysis
ðŸ“ˆ **Comprehensive Visualizations** - Score plots, feature importance, PCA projection
ðŸ”§ **Flexible** - Supports numeric and categorical variables

## Installation

```{r,eval=FALSE}
# Install from GitHub (development version)
# install.packages("remotes")
remotes::install_github("fabiandistler/outlieR")

# Install from CRAN (once available)
# install.packages("outlieR")
```

## Quick Start

```{r}
library(outlieR)

# Basic usage: Automatic outlier detection
result <- detect_outliers(mtcars)

# Display results
print(result)
summary(result)

# Extract outlier details
outlier_summary <- get_outlier_summary(result)
head(outlier_summary)

# Create visualizations
plot(result, type = "score") # Score distribution
plot(result, type = "features") # Feature importance
plot(result, type = "pca") # PCA projection
```

## Detailed Examples

### 1. Analyze specific columns

```{r}
# Use only selected variables
result <- detect_outliers(
  data = iris,
  target_cols = c("Sepal.Length", "Sepal.Width", "Petal.Length"),
  contamination = 0.05 # Expect 5% outliers
)
```

### 2. Without automatic tuning

```{r}
# Manually specify parameters for more control
result <- detect_outliers(
  data = mtcars,
  tune = FALSE,
  n_trees = 200,
  max_depth = 12,
  sample_size = 512
)
```

### 3. Different tuning methods

```{r,eval=FALSE}
# Grid Search (default, systematic but slower)
result_grid <- detect_outliers(mtcars, tune_method = "grid")

# Random Search (faster, good results)
result_random <- detect_outliers(mtcars, tune_method = "random")

# Bayesian Optimization (experimental)
result_bayes <- detect_outliers(mtcars, tune_method = "bayesian")
```

### 4. With categorical variables

```{r}
# Automatic one-hot encoding of factors
set.seed(42)
data <- data.frame(
  value1 = rnorm(100),
  value2 = rnorm(100),
  category = sample(c("A", "B", "C"), 100, replace = TRUE)
)

result <- detect_outliers(data)
```

### 5. Detailed outlier analysis

```{r,}
result <- detect_outliers(mtcars)

# Which features are suspicious in row 31?
detailed <- get_outlier_summary(result, detailed = TRUE)
detailed[row_id == 31]
```

## Performance

```{r,eval=FALSE}
# Benchmark on different data sizes
library(bench)

# Small dataset (1000 rows)
small_data <- data.frame(matrix(rnorm(1000 * 10), ncol = 10))
mark(detect_outliers(small_data, tune = FALSE))
# ~500ms

# Medium dataset (10000 rows)
medium_data <- data.frame(matrix(rnorm(10000 * 10), ncol = 10))
mark(detect_outliers(medium_data, tune = FALSE))
# ~2s

# With tuning (slower but better results)
mark(detect_outliers(medium_data, tune = TRUE, tune_method = "random"))
# ~20s (parallel on 4 cores)
```

## Advanced Usage

### Custom Threshold

```{r,eval=FALSE}
result <- detect_outliers(mtcars, contamination = 0.05)

# Use a custom threshold
custom_threshold <- quantile(result$scores, 0.99)
custom_outliers <- result$scores > custom_threshold

# Generate new details
result$outliers <- custom_outliers
result$threshold <- custom_threshold
```

## Troubleshooting

### Too many/few outliers

```{r,eval=FALSE}
# Adjust contamination parameter
result <- detect_outliers(data, contamination = 0.05) # Fewer outliers
result <- detect_outliers(data, contamination = 0.15) # More outliers
```

### Poor separation

```{r,eval=FALSE}
# Use more trees
result <- detect_outliers(data, n_trees = 300)

# Enable tuning for better parameters
result <- detect_outliers(data, tune = TRUE, tune_method = "random")
```

### Memory issues with large data

```{r,eval=FALSE}
# Disable parallel processing
result <- detect_outliers(large_data, parallel = FALSE)

# Fewer trees
result <- detect_outliers(large_data, n_trees = 50, tune = FALSE)
```

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a pull request

## License

MIT License â€“ see [LICENSE](LICENSE) file for details.

## Acknowledgements

* `isotree` package for the Isolation Forest implementation
* R community for feedback and inspiration
