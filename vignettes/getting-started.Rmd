---
title: "Getting Started with outlieR"
author: "Fabian Distler"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with outlieR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

# Introduction

`outlieR` provides a simple yet powerful interface for automatic outlier detection in tabular data. The package leverages Isolation Forests via the `isotree` package and extends it with:

* **Automatic hyperparameter tuning**
* **Detailed feature-level analysis**
* **Comprehensive visualizations**
* **data.table-based performance**

## Installation

```{r eval=FALSE}
# Development version from GitHub
remotes::install_github("fabiandistler/outlieR")

# CRAN (once available)
install.packages("outlieR")
```

```{r}
library(outlieR)
```

# Basic Usage

## Simplest Example

The main function `detect_outliers()` makes it extremely easy to find outliers:

```{r}
# Standard usage with mtcars dataset
result <- detect_outliers(mtcars, verbose = FALSE, parallel = FALSE)

# View result
print(result)
```

## Extracting Outlier Details

```{r}
# Summary of outliers
summary_dt <- get_outlier_summary(result, detailed = FALSE)
utils::head(summary_dt)

# Detailed information with feature scores
detailed_dt <- get_outlier_summary(result, detailed = TRUE)
utils::head(detailed_dt, 3)
```

## Visualizations

```{r fig.width=7, fig.height=5}
# Score distribution
plot(result, type = "score")
```

```{r fig.width=7, fig.height=5}
# Feature importance for outliers
plot(result, type = "features")
```

```{r fig.width=7, fig.height=5}
# Distribution of scores
plot(result, type = "distribution")
```

```{r fig.width=7, fig.height=5}
# PCA projection
plot(result, type = "pca")
```

# Advanced Usage

## Analyze Specific Columns

```{r,eval=FALSE}
# Use only selected features
result_subset <- detect_outliers(
  data = iris,
  target_cols = c("Sepal.Length", "Sepal.Width", "Petal.Length"),
  contamination = 0.05, # Expect 5% outliers
  verbose = FALSE
)

get_outlier_summary(result_subset, detailed = FALSE)
```

## Automatic Hyperparameter Tuning

```{r,eval=FALSE}
# Grid Search (systematic, all combinations)
result_grid <- detect_outliers(
  mtcars,
  tune = TRUE,
  tune_method = "grid",
  parallel = TRUE,
  verbose = FALSE
)

# Random Search (faster, good results)
result_random <- detect_outliers(
  mtcars,
  tune = TRUE,
  tune_method = "random",
  parallel = TRUE,
  verbose = FALSE
)

# Compare parameters
cat("Grid Search Parameters:\n")
print(result_grid$params)

cat("\nRandom Search Parameters:\n")
print(result_random$params)
```

## Manual Parameter Control

For maximum control, you can disable tuning:

```{r}
result_manual <- detect_outliers(
  mtcars,
  tune = FALSE,
  n_trees = 200,
  sample_size = 512,
  max_depth = 12,
  verbose = FALSE
)

print(result_manual$params)
```

# Working with Categorical Variables

`outlieR` automatically handles categorical variables via one-hot encoding:

```{r}
# Example dataset with categorical variables
set.seed(123)
mixed_data <- data.frame(
  numeric1 = rnorm(100),
  numeric2 = rnorm(100),
  category = sample(c("A", "B", "C"), 100, replace = TRUE),
  binary = sample(c("Yes", "No"), 100, replace = TRUE)
)

# Automatic detection and encoding
result_mixed <- detect_outliers(mixed_data, verbose = FALSE, parallel = FALSE)

# Display preprocessing information
cat("Original columns:", result_mixed$preprocessing$original_cols, "\n")
cat("Categorical columns:", result_mixed$preprocessing$categorical_cols, "\n")
cat("Processed columns:", result_mixed$preprocessing$processed_cols, "\n")
```


# Practical Workflows

## Workflow 1: Data Quality Check

```{r,eval=FALSE}
# Function for quick quality check
quality_check <- function(data, contamination = 0.05) {
  result <- detect_outliers(
    data,
    contamination = contamination,
    tune = FALSE, # Faster for quick check
    verbose = FALSE
  )

  list(
    n_outliers = sum(result$outliers),
    pct_outliers = 100 * mean(result$outliers),
    top_outliers = get_outlier_summary(result, detailed = FALSE)[1:5],
    metrics = result$metrics
  )
}

# Apply
qc_result <- quality_check(mtcars)
print(qc_result$top_outliers)
```

## Workflow 2: Iterative Cleaning

```{r,eval=FALSE}
# Iteratively remove outliers and reassess
iterative_cleaning <- function(data, max_iterations = 3, contamination = 0.1) {
  clean_data <- data
  removed_indices <- integer()

  for (i in seq_len(max_iterations)) {
    result <- detect_outliers(
      clean_data,
      contamination = contamination,
      tune = FALSE,
      verbose = FALSE
    )

    n_outliers <- sum(result$outliers)
    if (n_outliers == 0) break

    # Track indices in original dataset
    outlier_rows <- which(result$outliers)
    removed_indices <- c(removed_indices, outlier_rows)

    # Remove outliers
    clean_data <- clean_data[!result$outliers, ]

    cat(sprintf("Iteration %d: %d outliers removed\n", i, n_outliers))
  }

  list(
    clean_data = clean_data,
    removed_indices = removed_indices,
    n_removed = length(removed_indices)
  )
}

# Example (with small contamination value)
cleaned <- iterative_cleaning(mtcars, contamination = 0.05)
cat(sprintf("\nTotal removed: %d rows\n", cleaned$n_removed))
```

## Workflow 3: Feature-Specific Analysis

```{r, eval=FALSE}
# Which features are most frequently flagged as outliers?
analyze_feature_outliers <- function(result) {
  dt <- data.table::as.data.table(result$outlier_details)

  # Extract feature score columns
  score_cols <- grep("^feat_score_", names(dt), value = TRUE)

  if (length(score_cols) == 0) {
    return(NULL)
  }

  # For each feature: count observations with high score (>3)
  feature_stats <- lapply(score_cols, function(col) {
    col_clean <- gsub("^feat_score_", "", col)
    scores <- dt[[col]]
    data.table::data.table(
      feature = col_clean,
      n_high_score = sum(scores > 3, na.rm = TRUE),
      mean_score = mean(scores, na.rm = TRUE),
      max_score = max(scores, na.rm = TRUE)
    )
  })

  feature_dt <- data.table::rbindlist(feature_stats)
  feature_dt[order(-n_high_score)]
}

result <- outlieR::detect_outliers(mtcars, verbose = FALSE)
feature_analysis <- analyze_feature_outliers(result)
utils::head(feature_analysis, 10)
```

# Performance Tips

## For Large Datasets

```{r eval=FALSE}
# For large data (>100k rows):
result <- detect_outliers(
  large_data,
  tune = FALSE, # No tuning for speed
  n_trees = 100, # Moderate number of trees
  sample_size = 2048, # Larger samples
  parallel = TRUE, # Parallel processing
  verbose = TRUE # Progress tracking
)
```

## Optimize Tuning

```{r eval=FALSE}
# Fast tuning with random search
result <- detect_outliers(
  data,
  tune = TRUE,
  tune_method = "random", # Faster than grid
  parallel = TRUE,
  verbose = TRUE
)
```

## Memory Management

```{r eval=FALSE}
# For very large datasets: use data.table
library(data.table)
dt <- fread("large_file.csv") # Efficient loading

result <- detect_outliers(
  dt,
  target_cols = c("col1", "col2", "col3"), # Only relevant columns
  tune = FALSE,
  verbose = TRUE
)

# Original data is not stored in the result for >10k rows
```

# Metrics and Interpretation

## Understanding Anomaly Scores

```{r}
result <- detect_outliers(mtcars, verbose = FALSE, parallel = FALSE)

# Score statistics
cat("Score range:", range(result$scores), "\n")
cat("Score mean:", mean(result$scores), "\n")
cat("Score SD:", stats::sd(result$scores), "\n")
cat("Threshold:", result$threshold, "\n")

# View metrics
print(result$metrics)
```

### Interpreting Metrics

* **mean_score**: Average anomaly score
* **cohens_d**: Effect size of separation (>0.8 = strong)
* **outlier_separation**: Normalized separation between outliers and normal points
* **detection_rate**: Actual proportion of detected outliers


# Common Use Cases

## 1. Fraud Detection

```{r eval=FALSE}
# Check transaction data for suspicious patterns
transactions <- fread("transactions.csv")

result <- detect_outliers(
  transactions,
  target_cols = c("amount", "frequency", "location_distance"),
  contamination = 0.01, # Low for fraud (1%)
  tune = TRUE,
  verbose = TRUE
)

# Extract suspicious transactions for review
suspicious <- get_outlier_summary(result)
fwrite(suspicious, "suspicious_transactions.csv")
```

## 2. Sensor Data Validation

```{r eval=FALSE}
# Check IoT sensor readings
sensor_data <- data.table::fread("sensor_readings.csv")

result <- detect_outliers(
  sensor_data,
  target_cols = c("temperature", "pressure", "humidity"),
  contamination = 0.05,
  tune = FALSE,
  n_trees = 200
)

# Identify faulty readings
faulty_readings <- sensor_data[result$outliers, ]
```

## 3. Customer Segmentation

```{r eval=FALSE}
# Find unusual customer behavior patterns
customer_data <- data.table::fread("customer_behavior.csv")

result <- detect_outliers(
  customer_data,
  target_cols = c("purchase_frequency", "avg_order_value", "days_since_last"),
  contamination = 0.10,
  tune = TRUE,
  tune_method = "random"
)

# Identify VIP or risk customers
unusual_customers <- customer_data[result$outliers, ]
```
